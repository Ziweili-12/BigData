{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: DataFrame API and Spark SQL\n",
    "\n",
    "There is an [accompanying reference notebook](reference/reference-dataframes.ipynb) that shows many DataFrame and SparkSQL examples from the book [Learning Pyspark by Denny Lee and Thomas Drabasz](https://learning.oreilly.com/library/view/learning-pyspark/9781786463708/)\n",
    "\n",
    "\n",
    "## Social characteristics of the Marvel Universe\n",
    "\n",
    "In this lab you be working with a dataset that was creted about the Marvel Comics universe. The source data is a text file that was created with a graph analysis library outside of Spark, so the text file contains a lot of information and is not in a format that is easy to query with SQL. You are going to use it in this lab to practice data ingestion, manipulation and analysis using both the DataFrame API and Spark SQL. You can see more about the source data [here](http://bioinfo.uib.es/~joemiro/marvel.html).\n",
    "\n",
    "## Understanding the data file\n",
    "\n",
    "As mentioned above, the data file contains information about Marvel characters, publications, and which character appeared in each publication. There are two sections in the file:\n",
    "\n",
    "- Vertices (section begins in line 1 with a header in the form of `*Vertices 19428 6486`):\n",
    "    - Characters: lines 2-6487 in the format `6481 \"DETHSTRYK\"`, where `6481` is the node id and the name is within double quotes\n",
    "    - Publications: lines 6488-19429 in the same format as characters\n",
    "- Edgeslist (section begins in line 19430 with a header in the form of `*Edgeslist`): lines 19431 to the end of the file. The edge information is in the following format: `2 6488 6489 6490 6491 6492 6493 6494 6495 6496`. This represents a relationship between the character id (the first number) and the publication id's (all other numbers.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all your Spark related environment variables, and pyspark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your SparkSession. You are only going to create a `SparkSession`, not a `SparkContext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "     .appName(\"Test SparkSession\") \\\n",
    "     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your SparkSession is active:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-71-28.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Test SparkSession</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fba4435c650>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data.\n",
    "\n",
    "Although the data is a flat file with the structure explained earlier, you will be working with this file using [Spark DataFrame API and Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html).\n",
    "\n",
    "Load in the text file using [Generic load functions for SparkSQL](https://spark.apache.org/docs/2.3.0/sql-programming-guide.html#data-sources), which is located at `s3://bigdatateaching/marvel/porgat.txt`. This file is also located in Azure Blob at `wasbs://marvel@bigdatateaching.blob.core.windows.net/porgat.txt`. \n",
    "\n",
    "Create a DataFrame called `df_in` with a single field, where each record is the full text per line in the original text file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in = spark.read.text(\"s3://bigdatateaching/marvel/porgat.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the first few lines of `df_in`. What is the default field name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|*Vertices 19428 6486|\n",
      "|1 \"24-HOUR MAN/EM...|\n",
      "|2 \"3-D MAN/CHARLE...|\n",
      "|3 \"4-D MAN/MERCURIO\"|\n",
      "|         4 \"8-BALL/\"|\n",
      "|               5 \"A\"|\n",
      "|           6 \"A'YIN\"|\n",
      "|    7 \"ABBOTT, JACK\"|\n",
      "|         8 \"ABCISSA\"|\n",
      "|            9 \"ABEL\"|\n",
      "|10 \"ABOMINATION/E...|\n",
      "|11 \"ABOMINATION |...|\n",
      "|    12 \"ABOMINATRIX\"|\n",
      "|        13 \"ABRAXAS\"|\n",
      "|     14 \"ADAM 3,031\"|\n",
      "|        15 \"ABSALOM\"|\n",
      "|16 \"ABSORBING MAN...|\n",
      "|17 \"ABSORBING MAN...|\n",
      "|           18 \"ACBA\"|\n",
      "|19 \"ACHEBE, REVER...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_in.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of rows in `df_in`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30520"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens to a DataFrame when you bring it into your Python session from the cluster? Get the first few records of the `df_in` and save it into an object in your Python session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_10_python = df_in.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the local Python object. How is it different than the DataFrame in the cluster? Read up on the [Pyspark Row object](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=row#pyspark.sql.Row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='*Vertices 19428 6486'),\n",
       " Row(value='1 \"24-HOUR MAN/EMMANUEL\"'),\n",
       " Row(value='2 \"3-D MAN/CHARLES CHAN\"'),\n",
       " Row(value='3 \"4-D MAN/MERCURIO\"'),\n",
       " Row(value='4 \"8-BALL/\"'),\n",
       " Row(value='5 \"A\"'),\n",
       " Row(value='6 \"A\\'YIN\"'),\n",
       " Row(value='7 \"ABBOTT, JACK\"'),\n",
       " Row(value='8 \"ABCISSA\"'),\n",
       " Row(value='9 \"ABEL\"')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in_10_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_in_10_python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrange the data\n",
    "\n",
    "Since the data is in text format, at the end of this section you will have two DataFrames: one for nodes (both characters and publications) and one for edges (the relationship between characters and publications.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame API and SparkSQL functions are in the [pyspark.sql.functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions) library. You can import all of the functions or specific functions as needed.\n",
    "\n",
    "Start by importing the `monotonically_increasing_id` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new dataframe called `df_in_idx` where you will add a new field called `idx` that uses the `monotonically_increasing_id` to add a unique incremental numeric index to each record. You should read more about the function and understand how it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_idx = df_in.withColumn(\"idx\",monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See your new dataframe, you will see the new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|               value|idx|\n",
      "+--------------------+---+\n",
      "|*Vertices 19428 6486|  0|\n",
      "|1 \"24-HOUR MAN/EM...|  1|\n",
      "|2 \"3-D MAN/CHARLE...|  2|\n",
      "|3 \"4-D MAN/MERCURIO\"|  3|\n",
      "|         4 \"8-BALL/\"|  4|\n",
      "|               5 \"A\"|  5|\n",
      "|           6 \"A'YIN\"|  6|\n",
      "|    7 \"ABBOTT, JACK\"|  7|\n",
      "|         8 \"ABCISSA\"|  8|\n",
      "|            9 \"ABEL\"|  9|\n",
      "+--------------------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_in_idx.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know from the data file that the two headers are in lines 1 and 19430, and we want those lines (records) from the data. Create a new dataframe called `df_idx_no_hdr` where using a sql query, you select all records but those with the header.\n",
    "\n",
    "Before you can run a SparkSQL query, you need to \"register\" a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_idx.createOrReplaceTempView(\"df_in_idx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               value|  idx|\n",
      "+--------------------+-----+\n",
      "|*Vertices 19428 6486|    0|\n",
      "|          *Edgeslist|19429|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_in_idx.filter(df_in.value.rlike('^\\*')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idx_no_hdr = spark.sql(\"select * from df_in_idx where idx != 0 and idx!= 19429\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the header rows were removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30518"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idx_no_hdr.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idx_no_hdr.createOrReplaceTempView(\"df_idx_no_hdr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will create three separate dataframes: a `characters` one, a `publications` one, and a `relationships` one. You know the original line indices that partition the file, so use those. You have the `idx` field to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = spark.sql(\"select * from df_idx_no_hdr where idx between 1 and 6486\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications = spark.sql(\"select * from df_idx_no_hdr where idx between 6487 and 19428\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships = spark.sql(\"select * from df_idx_no_hdr where idx >19428\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6486"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|               value|idx|\n",
      "+--------------------+---+\n",
      "|1 \"24-HOUR MAN/EM...|  1|\n",
      "|2 \"3-D MAN/CHARLE...|  2|\n",
      "|3 \"4-D MAN/MERCURIO\"|  3|\n",
      "|         4 \"8-BALL/\"|  4|\n",
      "|               5 \"A\"|  5|\n",
      "|           6 \"A'YIN\"|  6|\n",
      "|    7 \"ABBOTT, JACK\"|  7|\n",
      "|         8 \"ABCISSA\"|  8|\n",
      "|            9 \"ABEL\"|  9|\n",
      "|10 \"ABOMINATION/E...| 10|\n",
      "|11 \"ABOMINATION |...| 11|\n",
      "|    12 \"ABOMINATRIX\"| 12|\n",
      "|        13 \"ABRAXAS\"| 13|\n",
      "|     14 \"ADAM 3,031\"| 14|\n",
      "|        15 \"ABSALOM\"| 15|\n",
      "|16 \"ABSORBING MAN...| 16|\n",
      "|17 \"ABSORBING MAN...| 17|\n",
      "|           18 \"ACBA\"| 18|\n",
      "|19 \"ACHEBE, REVER...| 19|\n",
      "|       20 \"ACHILLES\"| 20|\n",
      "+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "characters.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12942"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publications.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+\n",
      "|value          |idx |\n",
      "+---------------+----+\n",
      "|6487 \"AA2 35\"  |6487|\n",
      "|6488 \"M/PRM 35\"|6488|\n",
      "|6489 \"M/PRM 36\"|6489|\n",
      "|6490 \"M/PRM 37\"|6490|\n",
      "|6491 \"WI? 9\"   |6491|\n",
      "|6492 \"AVF 4\"   |6492|\n",
      "|6493 \"AVF 5\"   |6493|\n",
      "|6494 \"H2 251\"  |6494|\n",
      "|6495 \"H2 252\"  |6495|\n",
      "|6496 \"COC 1\"   |6496|\n",
      "|6497 \"T 208\"   |6497|\n",
      "|6498 \"T 214\"   |6498|\n",
      "|6499 \"T 215\"   |6499|\n",
      "|6500 \"T 216\"   |6500|\n",
      "|6501 \"T 440\"   |6501|\n",
      "|6502 \"CM 51\"   |6502|\n",
      "|6503 \"Q 14\"    |6503|\n",
      "|6504 \"Q 16\"    |6504|\n",
      "|6505 \"CA3 36\"  |6505|\n",
      "|6506 \"SLEEP 2\" |6506|\n",
      "+---------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "publications.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11090"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relationships.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               value|  idx|\n",
      "+--------------------+-----+\n",
      "|              1 6487|19430|\n",
      "|2 6488 6489 6490 ...|19431|\n",
      "|3 6497 6498 6499 ...|19432|\n",
      "|    4 6506 6507 6508|19433|\n",
      "|    5 6509 6510 6511|19434|\n",
      "|6 6512 6513 6514 ...|19435|\n",
      "|              7 6516|19436|\n",
      "|         8 6517 6518|19437|\n",
      "|         9 6519 6520|19438|\n",
      "|10 6521 6522 6523...|19439|\n",
      "|10 6536 6537 6538...|19440|\n",
      "|10 6551 6552 6553...|19441|\n",
      "|             11 6566|19442|\n",
      "|   12 6567 6568 6569|19443|\n",
      "|13 6570 6571 6572...|19444|\n",
      "|14 6574 6575 6576...|19445|\n",
      "|15 6578 6579 6580...|19446|\n",
      "|16 6582 6583 6584...|19447|\n",
      "|16 6597 6598 6599...|19448|\n",
      "|16 6612 6613 6614...|19449|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "relationships.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the characters and publications dataframes in order to run SQL commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters.createOrReplaceTempView(\"characters\")\n",
    "publications.createOrReplaceTempView(\"publications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `regexp_extract` function within a SQL statement and a regular expression to create three fields from the whole line: the first field will be the integer (which is the node id), the second is the text **between** the double quotes, and the third wether it is a character or publication. Create two new separate datframes, one for characters and the other for publications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char = spark.sql(\"\"\"select REGEXP_EXTRACT(value,''' \"(.*)\"''', 1) as name ,\\\n",
    "                  regexp_extract(value, '[0-9]+',0) as node_id,\\\n",
    "                   IF(idx <6487, \"Character\", \"Publication\") as Type\\\n",
    "          from characters\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pub = spark.sql(\"\"\"select REGEXP_EXTRACT(value,''' \"(.*)\"''', 1) as name ,\\\n",
    "                  regexp_extract(value, '[0-9]+',0) as node_id,\\\n",
    "                   IF(idx <6487, \"Character\", \"Publication\") as Type\\\n",
    "          from publications\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+---------+\n",
      "|name                |node_id|Type     |\n",
      "+--------------------+-------+---------+\n",
      "|24-HOUR MAN/EMMANUEL|1      |Character|\n",
      "|3-D MAN/CHARLES CHAN|2      |Character|\n",
      "|4-D MAN/MERCURIO    |3      |Character|\n",
      "|8-BALL/             |4      |Character|\n",
      "|A                   |5      |Character|\n",
      "|A'YIN               |6      |Character|\n",
      "|ABBOTT, JACK        |7      |Character|\n",
      "|ABCISSA             |8      |Character|\n",
      "|ABEL                |9      |Character|\n",
      "|ABOMINATION/EMIL BLO|10     |Character|\n",
      "|ABOMINATION | MUTANT|11     |Character|\n",
      "|ABOMINATRIX         |12     |Character|\n",
      "|ABRAXAS             |13     |Character|\n",
      "|ADAM 3,031          |14     |Character|\n",
      "|ABSALOM             |15     |Character|\n",
      "|ABSORBING MAN/CARL C|16     |Character|\n",
      "|ABSORBING MAN | MUTA|17     |Character|\n",
      "|ACBA                |18     |Character|\n",
      "|ACHEBE, REVEREND DOC|19     |Character|\n",
      "|ACHILLES            |20     |Character|\n",
      "+--------------------+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_char.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----------+\n",
      "|name    |node_id|Type       |\n",
      "+--------+-------+-----------+\n",
      "|AA2 35  |6487   |Publication|\n",
      "|M/PRM 35|6488   |Publication|\n",
      "|M/PRM 36|6489   |Publication|\n",
      "|M/PRM 37|6490   |Publication|\n",
      "|WI? 9   |6491   |Publication|\n",
      "|AVF 4   |6492   |Publication|\n",
      "|AVF 5   |6493   |Publication|\n",
      "|H2 251  |6494   |Publication|\n",
      "|H2 252  |6495   |Publication|\n",
      "|COC 1   |6496   |Publication|\n",
      "|T 208   |6497   |Publication|\n",
      "|T 214   |6498   |Publication|\n",
      "|T 215   |6499   |Publication|\n",
      "|T 216   |6500   |Publication|\n",
      "|T 440   |6501   |Publication|\n",
      "|CM 51   |6502   |Publication|\n",
      "|Q 14    |6503   |Publication|\n",
      "|Q 16    |6504   |Publication|\n",
      "|CA3 36  |6505   |Publication|\n",
      "|SLEEP 2 |6506   |Publication|\n",
      "+--------+-------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pub.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack both the character and publications into a single dataframe called `nodes_df`, and cache it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df = df_char.union(df_pub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+---------+\n",
      "|                name|node_id|     Type|\n",
      "+--------------------+-------+---------+\n",
      "|24-HOUR MAN/EMMANUEL|      1|Character|\n",
      "|3-D MAN/CHARLES CHAN|      2|Character|\n",
      "|    4-D MAN/MERCURIO|      3|Character|\n",
      "|             8-BALL/|      4|Character|\n",
      "|                   A|      5|Character|\n",
      "|               A'YIN|      6|Character|\n",
      "|        ABBOTT, JACK|      7|Character|\n",
      "|             ABCISSA|      8|Character|\n",
      "|                ABEL|      9|Character|\n",
      "|ABOMINATION/EMIL BLO|     10|Character|\n",
      "|ABOMINATION | MUTANT|     11|Character|\n",
      "|         ABOMINATRIX|     12|Character|\n",
      "|             ABRAXAS|     13|Character|\n",
      "|          ADAM 3,031|     14|Character|\n",
      "|             ABSALOM|     15|Character|\n",
      "|ABSORBING MAN/CARL C|     16|Character|\n",
      "|ABSORBING MAN | MUTA|     17|Character|\n",
      "|                ACBA|     18|Character|\n",
      "|ACHEBE, REVEREND DOC|     19|Character|\n",
      "|            ACHILLES|     20|Character|\n",
      "+--------------------+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nodes_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will work on the relationships dataframe. Import the `split` and `explode` functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships.createOrReplaceTempView(\"relationships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = relationships.withColumn(\"newcol\", split(col(\"value\"), \" \")).cache() #convert value to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|               value|  idx|              newcol|\n",
      "+--------------------+-----+--------------------+\n",
      "|              1 6487|19430|           [1, 6487]|\n",
      "|2 6488 6489 6490 ...|19431|[2, 6488, 6489, 6...|\n",
      "|3 6497 6498 6499 ...|19432|[3, 6497, 6498, 6...|\n",
      "|    4 6506 6507 6508|19433|[4, 6506, 6507, 6...|\n",
      "|    5 6509 6510 6511|19434|[5, 6509, 6510, 6...|\n",
      "|6 6512 6513 6514 ...|19435|[6, 6512, 6513, 6...|\n",
      "|              7 6516|19436|           [7, 6516]|\n",
      "|         8 6517 6518|19437|     [8, 6517, 6518]|\n",
      "|         9 6519 6520|19438|     [9, 6519, 6520]|\n",
      "|10 6521 6522 6523...|19439|[10, 6521, 6522, ...|\n",
      "|10 6536 6537 6538...|19440|[10, 6536, 6537, ...|\n",
      "|10 6551 6552 6553...|19441|[10, 6551, 6552, ...|\n",
      "|             11 6566|19442|          [11, 6566]|\n",
      "|   12 6567 6568 6569|19443|[12, 6567, 6568, ...|\n",
      "|13 6570 6571 6572...|19444|[13, 6570, 6571, ...|\n",
      "|14 6574 6575 6576...|19445|[14, 6574, 6575, ...|\n",
      "|15 6578 6579 6580...|19446|[15, 6578, 6579, ...|\n",
      "|16 6582 6583 6584...|19447|[16, 6582, 6583, ...|\n",
      "|16 6597 6598 6599...|19448|[16, 6597, 6598, ...|\n",
      "|16 6612 6613 6614...|19449|[16, 6612, 6613, ...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using a chain of daframe api methods, start with the value field in the `relationships` dataframe, wherw you will create a dataframe that takes the first value of the edge and puts it in the character field, and then \"explodes\" the other values in the publication column (1 row per character-publication combination.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+---------+\n",
      "|               value|  idx|              newcol|character|\n",
      "+--------------------+-----+--------------------+---------+\n",
      "|              1 6487|19430|           [1, 6487]|        1|\n",
      "|2 6488 6489 6490 ...|19431|[2, 6488, 6489, 6...|        2|\n",
      "|3 6497 6498 6499 ...|19432|[3, 6497, 6498, 6...|        3|\n",
      "|    4 6506 6507 6508|19433|[4, 6506, 6507, 6...|        4|\n",
      "|    5 6509 6510 6511|19434|[5, 6509, 6510, 6...|        5|\n",
      "|6 6512 6513 6514 ...|19435|[6, 6512, 6513, 6...|        6|\n",
      "|              7 6516|19436|           [7, 6516]|        7|\n",
      "|         8 6517 6518|19437|     [8, 6517, 6518]|        8|\n",
      "|         9 6519 6520|19438|     [9, 6519, 6520]|        9|\n",
      "|10 6521 6522 6523...|19439|[10, 6521, 6522, ...|       10|\n",
      "|10 6536 6537 6538...|19440|[10, 6536, 6537, ...|       10|\n",
      "|10 6551 6552 6553...|19441|[10, 6551, 6552, ...|       10|\n",
      "|             11 6566|19442|          [11, 6566]|       11|\n",
      "|   12 6567 6568 6569|19443|[12, 6567, 6568, ...|       12|\n",
      "|13 6570 6571 6572...|19444|[13, 6570, 6571, ...|       13|\n",
      "|14 6574 6575 6576...|19445|[14, 6574, 6575, ...|       14|\n",
      "|15 6578 6579 6580...|19446|[15, 6578, 6579, ...|       15|\n",
      "|16 6582 6583 6584...|19447|[16, 6582, 6583, ...|       16|\n",
      "|16 6597 6598 6599...|19448|[16, 6597, 6598, ...|       16|\n",
      "|16 6612 6613 6614...|19449|[16, 6612, 6613, ...|       16|\n",
      "+--------------------+-----+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Extract first value(character), and put these in a column called \"Character\"\n",
    "r21 = r2.withColumn(\"character\",r2.newcol[0])\n",
    "r21.createOrReplaceTempView(\"r21\")\n",
    "r21.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract rest values from **values**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "r23 = r2.rdd.map(lambda x: (x.idx,x.newcol[1:])).toDF()\n",
    "r23 = r23.toDF(\"ID\",\"Char\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "r23.createOrReplaceTempView(\"r23\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join two dataframe together and sort by the `idx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "r24 = r21.join(r23,r21.idx==r23.ID,how=\"left\").sort(col(\"idx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop some duplicate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "r25 = r24.drop(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df = r25.withColumn(\"publication\",explode(r25.Char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+---------+--------------------+-----------+\n",
      "|               value|  idx|              newcol|character|                Char|publication|\n",
      "+--------------------+-----+--------------------+---------+--------------------+-----------+\n",
      "|              1 6487|19430|           [1, 6487]|        1|              [6487]|       6487|\n",
      "|2 6488 6489 6490 ...|19431|[2, 6488, 6489, 6...|        2|[6488, 6489, 6490...|       6488|\n",
      "|2 6488 6489 6490 ...|19431|[2, 6488, 6489, 6...|        2|[6488, 6489, 6490...|       6489|\n",
      "|2 6488 6489 6490 ...|19431|[2, 6488, 6489, 6...|        2|[6488, 6489, 6490...|       6490|\n",
      "|2 6488 6489 6490 ...|19431|[2, 6488, 6489, 6...|        2|[6488, 6489, 6490...|       6491|\n",
      "|2 6488 6489 6490 ...|19431|[2, 6488, 6489, 6...|        2|[6488, 6489, 6490...|       6492|\n",
      "|2 6488 6489 6490 ...|19431|[2, 6488, 6489, 6...|        2|[6488, 6489, 6490...|       6493|\n",
      "|2 6488 6489 6490 ...|19431|[2, 6488, 6489, 6...|        2|[6488, 6489, 6490...|       6494|\n",
      "|2 6488 6489 6490 ...|19431|[2, 6488, 6489, 6...|        2|[6488, 6489, 6490...|       6495|\n",
      "|2 6488 6489 6490 ...|19431|[2, 6488, 6489, 6...|        2|[6488, 6489, 6490...|       6496|\n",
      "|3 6497 6498 6499 ...|19432|[3, 6497, 6498, 6...|        3|[6497, 6498, 6499...|       6497|\n",
      "|3 6497 6498 6499 ...|19432|[3, 6497, 6498, 6...|        3|[6497, 6498, 6499...|       6498|\n",
      "|3 6497 6498 6499 ...|19432|[3, 6497, 6498, 6...|        3|[6497, 6498, 6499...|       6499|\n",
      "|3 6497 6498 6499 ...|19432|[3, 6497, 6498, 6...|        3|[6497, 6498, 6499...|       6500|\n",
      "|3 6497 6498 6499 ...|19432|[3, 6497, 6498, 6...|        3|[6497, 6498, 6499...|       6501|\n",
      "|3 6497 6498 6499 ...|19432|[3, 6497, 6498, 6...|        3|[6497, 6498, 6499...|       6502|\n",
      "|3 6497 6498 6499 ...|19432|[3, 6497, 6498, 6...|        3|[6497, 6498, 6499...|       6503|\n",
      "|3 6497 6498 6499 ...|19432|[3, 6497, 6498, 6...|        3|[6497, 6498, 6499...|       6504|\n",
      "|3 6497 6498 6499 ...|19432|[3, 6497, 6498, 6...|        3|[6497, 6498, 6499...|       6505|\n",
      "|    4 6506 6507 6508|19433|[4, 6506, 6507, 6...|        4|  [6506, 6507, 6508]|       6506|\n",
      "+--------------------+-----+--------------------+---------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edges_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an analytical dataset\n",
    "\n",
    "You will now create an analytical dataset using SparkSQL where you will join both tables (nodes_df and edge_df) so you have the data you need to run some analytics on the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df.createOrReplaceTempView(\"edges_df\")\n",
    "nodes_df.createOrReplaceTempView(\"nodes_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytics = spark.sql(\"\"\"select e.character as character_id, c.name as character, \n",
    "                      e.publication as publication_id, p.name as publication\n",
    "                      from edges_df e \n",
    "                      left join nodes_df c on e.character = c.node_id\n",
    "                      left join nodes_df p on e.publication = p.node_id\n",
    "                      \"\"\").cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------+-----------+\n",
      "|character_id|           character|publication_id|publication|\n",
      "+------------+--------------------+--------------+-----------+\n",
      "|           1|24-HOUR MAN/EMMANUEL|          6487|     AA2 35|\n",
      "|           2|3-D MAN/CHARLES CHAN|          6488|   M/PRM 35|\n",
      "|           2|3-D MAN/CHARLES CHAN|          6489|   M/PRM 36|\n",
      "|           2|3-D MAN/CHARLES CHAN|          6490|   M/PRM 37|\n",
      "|           2|3-D MAN/CHARLES CHAN|          6491|      WI? 9|\n",
      "|           2|3-D MAN/CHARLES CHAN|          6492|      AVF 4|\n",
      "|           2|3-D MAN/CHARLES CHAN|          6493|      AVF 5|\n",
      "|           2|3-D MAN/CHARLES CHAN|          6494|     H2 251|\n",
      "|           2|3-D MAN/CHARLES CHAN|          6495|     H2 252|\n",
      "|           2|3-D MAN/CHARLES CHAN|          6496|      COC 1|\n",
      "+------------+--------------------+--------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analytics.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top 10 publications with the highest number of characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|         publication|char_ct|\n",
      "+--------------------+-------+\n",
      "|               COC 1|    111|\n",
      "|MARVEL MYSTERY COMIC|     92|\n",
      "|                IW 3|     91|\n",
      "|                IW 1|     90|\n",
      "|              H2 279|     87|\n",
      "|                IW 4|     80|\n",
      "|                IW 2|     76|\n",
      "|            MAXSEC 3|     72|\n",
      "|              FF 370|     63|\n",
      "|                IW 6|     60|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analytics.createOrReplaceTempView(\"analytics\")\n",
    "spark.sql(\"\"\"select publication, count(*) as char_ct from analytics \n",
    "            group by publication order by char_ct desc limit 10\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a DataFrame to a csv\n",
    "```\n",
    "df.write\\\n",
    "    .format(\"com.databricks.spark.csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .save(\"path-in-hdfs-or-cloud\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab is based on [https://vincentlauzon.com/2018/01/24/azure-databricks-spark-sql-data-frames/](https://vincentlauzon.com/2018/01/24/azure-databricks-spark-sql-data-frames/). You can see the code in that blog post to perform the same operations using RDD commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you close the Jupyter Notebook, it is best to close the connection to the Spark cluster. If you don't you may have an \"orphan\" connection that is eating up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
