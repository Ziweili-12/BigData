{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Working with SparkSQL\n",
    "\n",
    "Do not insert any additional cells than the ones that are provided.\n",
    "\n",
    "Create your SparkContext and SparkSession:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "     .appName(\"Test SparkSession\") \\\n",
    "     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quazyilx again!\n",
    "\n",
    "Yes, you remember it. As a reminder, here is the description of the files.\n",
    "\n",
    "The quazyilx has been malfunctioning, and occasionally generates output with a `-1` for all four measurements, like this:\n",
    "\n",
    "    2015-12-10T08:40:10Z fnard:-1 fnok:-1 cark:-1 gnuck:-1\n",
    "\n",
    "There are four different versions of the _quazyilx_ file, each of a different size. As you can see in the output below the file sizes are 50MB (1,000,000 rows), 4.8GB (100,000,000 rows), 18GB (369,865,098 rows) and 36.7GB (752,981,134 rows). The only difference is the length of the number of records, the file structure is the same.\n",
    "\n",
    "```\n",
    "[hadoop@ip-172-31-1-240 ~]$ hadoop fs -ls s3://bigdatateaching/quazyilx/\n",
    "Found 4 items\n",
    "-rw-rw-rw-   1 hadoop hadoop    52443735 2018-01-25 15:37 s3://bigdatateaching/quazyilx/quazyilx0.txt\n",
    "-rw-rw-rw-   1 hadoop hadoop  5244417004 2018-01-25 15:37 s3://bigdatateaching/quazyilx/quazyilx1.txt\n",
    "-rw-rw-rw-   1 hadoop hadoop 19397230888 2018-01-25 15:38 s3://bigdatateaching/quazyilx/quazyilx2.txt\n",
    "-rw-rw-rw-   1 hadoop hadoop 39489364082 2018-01-25 15:41 s3://bigdatateaching/quazyilx/quazyilx3.txt\n",
    "```\n",
    "\n",
    "You will use Spark and SparkSQL to create a Spark DataFrame and then run some analysis on the files using SparkSQL queries.\n",
    "\n",
    "First, in the following cell, create an RDD called `quazyilx` that reads the `quazyilx1.txt` file from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "quazyilx = sc.textFile(\"s3://bigdatateaching/quazyilx/quazyilx1.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, look at the first 50 elements of `quazyilx` to make sure everything is working corectly. This should take a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2000-01-01 00:00:03 fnard:7 fnok:8 cark:19 gnuck:25',\n",
       " '2000-01-01 00:00:08 fnard:14 fnok:19 cark:16 gnuck:37',\n",
       " '2000-01-01 00:00:17 fnard:12 fnok:11 cark:12 gnuck:8',\n",
       " '2000-01-01 00:00:22 fnard:18 fnok:16 cark:3 gnuck:8',\n",
       " '2000-01-01 00:00:32 fnard:7 fnok:16 cark:7 gnuck:37',\n",
       " '2000-01-01 00:00:40 fnard:6 fnok:14 cark:3 gnuck:30',\n",
       " '2000-01-01 00:00:47 fnard:11 fnok:10 cark:17 gnuck:7',\n",
       " '2000-01-01 00:00:55 fnard:9 fnok:14 cark:13 gnuck:30',\n",
       " '2000-01-01 00:00:56 fnard:10 fnok:1 cark:7 gnuck:6',\n",
       " '2000-01-01 00:00:59 fnard:11 fnok:11 cark:12 gnuck:18',\n",
       " '2000-01-01 00:01:03 fnard:9 fnok:13 cark:14 gnuck:49',\n",
       " '2000-01-01 00:01:06 fnard:12 fnok:10 cark:19 gnuck:30',\n",
       " '2000-01-01 00:01:16 fnard:0 fnok:12 cark:19 gnuck:26',\n",
       " '2000-01-01 00:01:26 fnard:10 fnok:11 cark:10 gnuck:49',\n",
       " '2000-01-01 00:01:30 fnard:9 fnok:5 cark:16 gnuck:13',\n",
       " '2000-01-01 00:01:38 fnard:11 fnok:10 cark:7 gnuck:47',\n",
       " '2000-01-01 00:01:43 fnard:2 fnok:2 cark:20 gnuck:35',\n",
       " '2000-01-01 00:01:53 fnard:12 fnok:11 cark:20 gnuck:3',\n",
       " '2000-01-01 00:01:54 fnard:6 fnok:6 cark:16 gnuck:18',\n",
       " '2000-01-01 00:02:01 fnard:9 fnok:10 cark:17 gnuck:15',\n",
       " '2000-01-01 00:02:08 fnard:0 fnok:6 cark:3 gnuck:49',\n",
       " '2000-01-01 00:02:10 fnard:15 fnok:12 cark:3 gnuck:14',\n",
       " '2000-01-01 00:02:17 fnard:7 fnok:-1 cark:18 gnuck:25',\n",
       " '2000-01-01 00:02:26 fnard:10 fnok:0 cark:4 gnuck:26',\n",
       " '2000-01-01 00:02:35 fnard:4 fnok:17 cark:14 gnuck:6',\n",
       " '2000-01-01 00:02:43 fnard:10 fnok:15 cark:19 gnuck:2',\n",
       " '2000-01-01 00:02:50 fnard:8 fnok:16 cark:7 gnuck:1',\n",
       " '2000-01-01 00:02:55 fnard:17 fnok:4 cark:17 gnuck:23',\n",
       " '2000-01-01 00:03:05 fnard:7 fnok:14 cark:18 gnuck:8',\n",
       " '2000-01-01 00:03:10 fnard:12 fnok:19 cark:7 gnuck:43',\n",
       " '2000-01-01 00:03:15 fnard:4 fnok:4 cark:4 gnuck:50',\n",
       " '2000-01-01 00:03:17 fnard:17 fnok:4 cark:13 gnuck:22',\n",
       " '2000-01-01 00:03:27 fnard:19 fnok:1 cark:18 gnuck:0',\n",
       " '2000-01-01 00:03:32 fnard:10 fnok:11 cark:4 gnuck:40',\n",
       " '2000-01-01 00:03:40 fnard:5 fnok:10 cark:6 gnuck:45',\n",
       " '2000-01-01 00:03:46 fnard:19 fnok:20 cark:7 gnuck:36',\n",
       " '2000-01-01 00:03:49 fnard:7 fnok:15 cark:17 gnuck:44',\n",
       " '2000-01-01 00:03:52 fnard:7 fnok:8 cark:17 gnuck:11',\n",
       " '2000-01-01 00:03:59 fnard:1 fnok:4 cark:11 gnuck:21',\n",
       " '2000-01-01 00:04:09 fnard:12 fnok:-1 cark:0 gnuck:48',\n",
       " '2000-01-01 00:04:10 fnard:4 fnok:19 cark:16 gnuck:30',\n",
       " '2000-01-01 00:04:14 fnard:18 fnok:7 cark:5 gnuck:11',\n",
       " '2000-01-01 00:04:18 fnard:19 fnok:11 cark:3 gnuck:29',\n",
       " '2000-01-01 00:04:21 fnard:18 fnok:14 cark:14 gnuck:35',\n",
       " '2000-01-01 00:04:23 fnard:0 fnok:19 cark:20 gnuck:17',\n",
       " '2000-01-01 00:04:30 fnard:7 fnok:7 cark:6 gnuck:42',\n",
       " '2000-01-01 00:04:34 fnard:14 fnok:2 cark:13 gnuck:39',\n",
       " '2000-01-01 00:04:41 fnard:19 fnok:14 cark:12 gnuck:32',\n",
       " '2000-01-01 00:04:51 fnard:19 fnok:18 cark:20 gnuck:34',\n",
       " '2000-01-01 00:04:57 fnard:3 fnok:1 cark:20 gnuck:16']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quazyilx.take(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now need to work with the RDD to be able to make a DataFrame. In the following cell, create python class called `quazyilx_class` that processes a line and returns attributes for `.time`, `.fnard`, `.fnok` and `.cark`. \n",
    "\n",
    "You will need to define the Regular Expression and complete the class where it says `#Put your code here:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os,datetime,re\n",
    "\n",
    "QUAZYILX_RE = \"(....-..-.. ..:..:..) fnard:(-?[\\d]+) fnok:(-?[\\d]+) cark:(-?[\\d]+) gnuck:(-?[\\d]+)\"\n",
    "quazyilx_re = re.compile(QUAZYILX_RE)\n",
    "\n",
    "class quazyilx_class():\n",
    "    def __init__(self,line):\n",
    "        value = re.match(quazyilx_re,line)\n",
    "        try:\n",
    "            self.time = str(value.group(1))\n",
    "            self.fnard = int(value.group(2))\n",
    "            self.fnok = int(value.group(3)) \n",
    "            self.cark = int(value.group(4))\n",
    "            self.gnuck = int(value.group(5))\n",
    "            return\n",
    "        except:\n",
    "            self.time = None\n",
    "            self.fnard = None\n",
    "            self.fnok = None\n",
    "            self.cark = None\n",
    "            self.gnuck = None\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will then need to turn the quazyilx RDD into a `Row()` object. You can do that with a lambda function, like this:\n",
    "\n",
    "```(python)\n",
    "lambda q:Row(datetime=q.datetime.isoformat(),fnard=q.fnard,fnok=q.fnok,cark=q.cark,gnuck=q.gnuck))\n",
    "```\n",
    "\n",
    "Alternatively, you can add a new method to the Quazyilx class called `.Row()` that returns a Row. All of these ways are more or less equivalent. You just need to pick one of them.  You may find it useful to look at [this documentation](http://spark.apache.org/docs/latest/sql-programming-guide.html#inferring-the-schema-using-reflection).\n",
    "\n",
    "In the next cell, create an RDD called `line` that converts the `quazyilx` RDD into a `Row()` object using the `quazyilx_class`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "line = quazyilx.map(lambda line:Row(time = datetime.datetime.strptime(quazyilx_class(line).time,\"%Y-%m-%d %H:%M:%S\").isoformat(),\n",
    "                            fnard = quazyilx_class(line).fnard,\n",
    "                            fnok = quazyilx_class(line).fnok,\n",
    "                            cark = quazyilx_class(line).cark,\n",
    "                            gnuck = quazyilx_class(line).gnuck))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the first 10 rows to make sure everything is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cark=19, fnard=7, fnok=8, gnuck=25, time='2000-01-01T00:00:03'),\n",
       " Row(cark=16, fnard=14, fnok=19, gnuck=37, time='2000-01-01T00:00:08'),\n",
       " Row(cark=12, fnard=12, fnok=11, gnuck=8, time='2000-01-01T00:00:17'),\n",
       " Row(cark=3, fnard=18, fnok=16, gnuck=8, time='2000-01-01T00:00:22'),\n",
       " Row(cark=7, fnard=7, fnok=16, gnuck=37, time='2000-01-01T00:00:32'),\n",
       " Row(cark=3, fnard=6, fnok=14, gnuck=30, time='2000-01-01T00:00:40'),\n",
       " Row(cark=17, fnard=11, fnok=10, gnuck=7, time='2000-01-01T00:00:47'),\n",
       " Row(cark=13, fnard=9, fnok=14, gnuck=30, time='2000-01-01T00:00:55'),\n",
       " Row(cark=7, fnard=10, fnok=1, gnuck=6, time='2000-01-01T00:00:56'),\n",
       " Row(cark=12, fnard=11, fnok=11, gnuck=18, time='2000-01-01T00:00:59')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, convert the quazyilx RDD into a DataFrame `quazyilx_df` using the `spark.createDataFrame` method, register it as the SQL table `quazyilx_tbl` with the method `.createOrReplaceTempView`. You will want to cache the DataFrame so it doesn't get generated every time you run a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+-----+-------------------+\n",
      "|cark|fnard|fnok|gnuck|               time|\n",
      "+----+-----+----+-----+-------------------+\n",
      "|  19|    7|   8|   25|2000-01-01T00:00:03|\n",
      "|  16|   14|  19|   37|2000-01-01T00:00:08|\n",
      "|  12|   12|  11|    8|2000-01-01T00:00:17|\n",
      "|   3|   18|  16|    8|2000-01-01T00:00:22|\n",
      "|   7|    7|  16|   37|2000-01-01T00:00:32|\n",
      "|   3|    6|  14|   30|2000-01-01T00:00:40|\n",
      "|  17|   11|  10|    7|2000-01-01T00:00:47|\n",
      "|  13|    9|  14|   30|2000-01-01T00:00:55|\n",
      "|   7|   10|   1|    6|2000-01-01T00:00:56|\n",
      "|  12|   11|  11|   18|2000-01-01T00:00:59|\n",
      "|  14|    9|  13|   49|2000-01-01T00:01:03|\n",
      "|  19|   12|  10|   30|2000-01-01T00:01:06|\n",
      "|  19|    0|  12|   26|2000-01-01T00:01:16|\n",
      "|  10|   10|  11|   49|2000-01-01T00:01:26|\n",
      "|  16|    9|   5|   13|2000-01-01T00:01:30|\n",
      "|   7|   11|  10|   47|2000-01-01T00:01:38|\n",
      "|  20|    2|   2|   35|2000-01-01T00:01:43|\n",
      "|  20|   12|  11|    3|2000-01-01T00:01:53|\n",
      "|  16|    6|   6|   18|2000-01-01T00:01:54|\n",
      "|  17|    9|  10|   15|2000-01-01T00:02:01|\n",
      "+----+-----+----+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quazyilx_df = spark.createDataFrame(line)\n",
    "quazyilx_df.createOrReplaceTempView(\"quazyilx_tbl\")\n",
    "quazyilx_df.cache()\n",
    "quazyilx_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you create and register the dataframe and table, you will run SQL queries using  `spark.sql()` to calculate the following:\n",
    "\n",
    "1. The number of rows in the dataset\n",
    "1. The number of lines that has -1 for `fnard`, `fnok`, `cark` and `gnuck`.\n",
    "1. The number of lines that have -1 for `fnard` but have `fnok > 5` and `cark > 5`\n",
    "1. The first datetime in the dataset\n",
    "1. The first datetime that has -1 for all of the values\n",
    "1. The last datetime in the dataset\n",
    "1. The last datetime that has a -1 for all of the values\n",
    "\n",
    "Place each query into each of the following  seven cells and run it to get the results. Remember, running the query statement itselft will not give you the results you want. You need to do something else to \"get\" the result.\n",
    "\n",
    "**Note: in development, the first query may take approximately 10-15 minutes to run with the cluster configuration for this assignment (1 master, 4 task nodes of m4.xlarge). If you cache() correctly, all subsequent queries should take no more than 5 seconds.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| count(1)|\n",
      "+---------+\n",
      "|100000000|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The number of rows in the dataset\n",
    "spark.sql(\"select count(*) from quazyilx_tbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     190|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of lines that has -1 for fnard,fnok,cark and gnuck\n",
    "spark.sql(\"\"\"select count(*) \n",
    "             from quazyilx_tbl \n",
    "             where fnard = -1 \n",
    "             AND fnok = -1 \n",
    "             AND cark = -1 \n",
    "             AND gnuck =-1\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 2114009|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of lines that have -1 for fnard but have fnok>5, cark>5\n",
    "spark.sql(\"\"\"select count(*) from quazyilx_tbl where fnard=-1 and fnok>5 and cark>5\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|         first_date|\n",
      "+-------------------+\n",
      "|2000-01-01T00:00:03|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first datetime in the dataset\n",
    "spark.sql(\"\"\"select min(time) as first_date from quazyilx_tbl \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+-----+-------------------+\n",
      "|cark|fnard|fnok|gnuck|               time|\n",
      "+----+-----+----+-----+-------------------+\n",
      "|  -1|   -1|  -1|   -1|2000-01-28T03:07:44|\n",
      "+----+-----+----+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first datetime that has -1 for all values\n",
    "spark.sql(\"\"\"select * \n",
    "             from quazyilx_tbl\n",
    "             where cark=-1\n",
    "             AND fnard=-1\n",
    "             AND fnok=-1\n",
    "             AND gnuck=-1\n",
    "             order by time ASC\n",
    "             limit 1\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          last_date|\n",
      "+-------------------+\n",
      "|2017-06-05T18:03:07|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# last datetime in the dataset\n",
    "spark.sql(\"\"\"select max(time) as last_date from quazyilx_tbl \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+-----+-------------------+\n",
      "|cark|fnard|fnok|gnuck|               time|\n",
      "+----+-----+----+-----+-------------------+\n",
      "|  -1|   -1|  -1|   -1|2017-04-21T04:57:10|\n",
      "+----+-----+----+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# last datetime that has -1 for all values\n",
    "spark.sql(\"\"\"select * \n",
    "             from quazyilx_tbl\n",
    "             where cark=-1\n",
    "             AND fnard=-1\n",
    "             AND fnok=-1\n",
    "             AND gnuck=-1\n",
    "             order by time DESC\n",
    "             limit 1\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you finish this problem, click on the File -> 'Save and Checkpoint' in the menu bar to make sure that the latest version of the workbook file is saved. Also, before you close this notebook and move on, make sure you disconnect your SparkContext, otherwise you will not be able to re-allocate resources. Remember, you will commit the .ipynb file to the repository for submission (in the master node terminal.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
